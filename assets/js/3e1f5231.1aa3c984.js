"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[238],{53252:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>i,metadata:()=>r,toc:()=>c});var o=t(85893),a=t(11151);const i={custom_edit_url:"https://github.com/autogenhub/autogen/edit/main/notebook/agentchat_langchain.ipynb",description:"Use Langchain tools.",source_notebook:"/notebook/agentchat_langchain.ipynb",tags:["langchain","integration","tool/function"],title:"Auto Generated Agent Chat: Task Solving with Langchain Provided Tools as Functions"},s="Auto Generated Agent Chat: Task Solving with Langchain Provided Tools as Functions",r={id:"notebooks/agentchat_langchain",title:"Auto Generated Agent Chat: Task Solving with Langchain Provided Tools as Functions",description:"Use Langchain tools.",source:"@site/docs/notebooks/agentchat_langchain.mdx",sourceDirName:"notebooks",slug:"/notebooks/agentchat_langchain",permalink:"/autogen/docs/notebooks/agentchat_langchain",draft:!1,unlisted:!1,editUrl:"https://github.com/autogenhub/autogen/edit/main/notebook/agentchat_langchain.ipynb",tags:[{label:"langchain",permalink:"/autogen/docs/tags/langchain"},{label:"integration",permalink:"/autogen/docs/tags/integration"},{label:"tool/function",permalink:"/autogen/docs/tags/tool-function"}],version:"current",frontMatter:{custom_edit_url:"https://github.com/autogenhub/autogen/edit/main/notebook/agentchat_langchain.ipynb",description:"Use Langchain tools.",source_notebook:"/notebook/agentchat_langchain.ipynb",tags:["langchain","integration","tool/function"],title:"Auto Generated Agent Chat: Task Solving with Langchain Provided Tools as Functions"},sidebar:"notebooksSidebar",previous:{title:"Auto Generated Agent Chat: Function Inception",permalink:"/autogen/docs/notebooks/agentchat_inception_function"},next:{title:"Engaging with Multimodal Models: GPT-4V in AutoGen",permalink:"/autogen/docs/notebooks/agentchat_lmm_gpt-4v"}},l={},c=[{value:"Requirements",id:"requirements",level:2},{value:"Set your API Endpoint",id:"set-your-api-endpoint",level:2},{value:"Making Function Calls",id:"making-function-calls",level:2}];function h(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,a.a)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.h1,{id:"auto-generated-agent-chat-task-solving-with-langchain-provided-tools-as-functions",children:"Auto Generated Agent Chat: Task Solving with Langchain Provided Tools as Functions"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.a,{href:"https://colab.research.google.com/github/autogenhub/autogen/blob/main/notebook/agentchat_langchain.ipynb",children:(0,o.jsx)(e.img,{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})}),"\n",(0,o.jsx)(e.a,{href:"https://github.com/autogenhub/autogen/blob/main/notebook/agentchat_langchain.ipynb",children:(0,o.jsx)(e.img,{src:"https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github",alt:"Open on GitHub"})})]}),"\n",(0,o.jsxs)(e.p,{children:["AutoGen offers conversable agents powered by LLM, tool, or human, which\ncan be used to perform tasks collectively via automated chat. This\nframework allows tool use and human participants through multi-agent\nconversation. Please find documentation about this feature\n",(0,o.jsx)(e.a,{href:"https://autogenhub.github.io/autogen/docs/Use-Cases/agent_chat",children:"here"}),"."]}),"\n",(0,o.jsxs)(e.p,{children:["In this notebook, we demonstrate how to use ",(0,o.jsx)(e.code,{children:"AssistantAgent"})," and\n",(0,o.jsx)(e.code,{children:"UserProxyAgent"})," to make function calls with the new feature of OpenAI\nmodels (in model version 0613) with a set of Langchain-provided tools\nand toolkits, to demonstrate how to leverage the 35+ tools available. A\nspecified prompt and function configs must be passed to ",(0,o.jsx)(e.code,{children:"AssistantAgent"}),"\nto initialize the agent. The corresponding functions must be passed to\n",(0,o.jsx)(e.code,{children:"UserProxyAgent"}),", which will execute any function calls made by\n",(0,o.jsx)(e.code,{children:"AssistantAgent"}),". Besides this requirement of matching descriptions with\nfunctions, we recommend checking the system message in the\n",(0,o.jsx)(e.code,{children:"AssistantAgent"})," to ensure the instructions align with the function call\ndescriptions."]}),"\n",(0,o.jsx)(e.h2,{id:"requirements",children:"Requirements"}),"\n",(0,o.jsxs)(e.p,{children:["AutoGen requires ",(0,o.jsx)(e.code,{children:"Python>=3.8"}),". To run this notebook example, please\ninstall ",(0,o.jsx)(e.code,{children:"pyautogen"})," and ",(0,o.jsx)(e.code,{children:"Langchain"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"pip install pyautogen Langchain\n"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'%pip install "pyautogen>=0.2.3" Langchain\n'})}),"\n",(0,o.jsx)(e.h2,{id:"set-your-api-endpoint",children:"Set your API Endpoint"}),"\n",(0,o.jsxs)(e.p,{children:["The\n",(0,o.jsx)(e.a,{href:"https://autogenhub.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_models",children:(0,o.jsx)(e.code,{children:"config_list_from_models"})}),"\nfunction tries to create a list of configurations using Azure OpenAI\nendpoints and OpenAI endpoints for the provided list of models. It\nassumes the api keys and api bases are stored in the corresponding\nenvironment variables or local txt files:"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:["OpenAI API key: os.environ[\u201cOPENAI_API_KEY\u201d] or\n",(0,o.jsx)(e.code,{children:'openai_api_key_file="key_openai.txt"'}),"."]}),"\n",(0,o.jsxs)(e.li,{children:["Azure OpenAI API key: os.environ[\u201cAZURE_OPENAI_API_KEY\u201d] or\n",(0,o.jsx)(e.code,{children:'aoai_api_key_file="key_aoai.txt"'}),". Multiple keys can be stored, one\nper line."]}),"\n",(0,o.jsxs)(e.li,{children:["Azure OpenAI API base: os.environ[\u201cAZURE_OPENAI_API_BASE\u201d] or\n",(0,o.jsx)(e.code,{children:'aoai_api_base_file="base_aoai.txt"'}),". Multiple bases can be stored,\none per line."]}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"It\u2019s OK to have only the OpenAI API key, or only the Azure OpenAI API\nkey + base. If you open this notebook in google colab, you can upload\nyour files by clicking the file icon on the left panel and then choosing\n\u201cupload file\u201d icon."}),"\n",(0,o.jsxs)(e.p,{children:["The following code excludes Azure OpenAI endpoints from the config list\nbecause some endpoints don\u2019t support functions yet. Remove the ",(0,o.jsx)(e.code,{children:"exclude"}),"\nargument if they do."]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import math\nimport os\nfrom typing import Optional, Type\n\n# Starndard Langchain example\nfrom langchain.agents import create_spark_sql_agent\nfrom langchain.agents.agent_toolkits import SparkSQLToolkit\nfrom langchain.chat_models import ChatOpenAI\n\n# Import things that are needed generically\nfrom langchain.pydantic_v1 import BaseModel, Field\nfrom langchain.tools import BaseTool\nfrom langchain.tools.file_management.read import ReadFileTool\nfrom langchain.utilities.spark_sql import SparkSQL\nfrom pyspark.sql import SparkSession\n\nimport autogen\n\nconfig_list = autogen.config_list_from_json(\n    "OAI_CONFIG_LIST",\n    filter_dict={\n        "model": ["gpt-4", "gpt-3.5-turbo", "gpt-3.5-turbo-16k"],\n    },\n)\n'})}),"\n",(0,o.jsx)(e.p,{children:"It first looks for environment variable \u201cOAI_CONFIG_LIST\u201d which needs to\nbe a valid json string. If that variable is not found, it then looks for\na json file named \u201cOAI_CONFIG_LIST\u201d. It filters the configs by models\n(you can filter by other keys as well). Only the models with matching\nnames are kept in the list based on the filter condition."}),"\n",(0,o.jsx)(e.p,{children:"The config list looks like the following:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"config_list = [\n    {\n        'model': 'gpt-4',\n        'api_key': '<your OpenAI API key here>',\n    },\n    {\n        'model': 'gpt-3.5-turbo',\n        'api_key': '<your Azure OpenAI API key here>',\n        'base_url': '<your Azure OpenAI API base here>',\n        'api_type': 'azure',\n        'api_version': '2024-02-15-preview',\n    },\n    {\n        'model': 'gpt-3.5-turbo-16k',\n        'api_key': '<your Azure OpenAI API key here>',\n        'base_url': '<your Azure OpenAI API base here>',\n        'api_type': 'azure',\n        'api_version': '2024-02-15-preview',\n    },\n]\n"})}),"\n",(0,o.jsxs)(e.p,{children:["You can set the value of config_list in any way you prefer. Please refer\nto this\n",(0,o.jsx)(e.a,{href:"https://github.com/autogenhub/autogen/blob/main/website/docs/topics/llm_configuration.ipynb",children:"notebook"}),"\nfor full code examples of the different methods."]}),"\n",(0,o.jsx)(e.h2,{id:"making-function-calls",children:"Making Function Calls"}),"\n",(0,o.jsxs)(e.p,{children:["In this example, we demonstrate function call execution with\n",(0,o.jsx)(e.code,{children:"AssistantAgent"})," and ",(0,o.jsx)(e.code,{children:"UserProxyAgent"}),". With the default system prompt of\n",(0,o.jsx)(e.code,{children:"AssistantAgent"}),", we allow the LLM assistant to perform tasks with code,\nand the ",(0,o.jsx)(e.code,{children:"UserProxyAgent"})," would extract code blocks from the LLM response\nand execute them. With the new \u201cfunction_call\u201d feature, we define\nfunctions and specify the description of the function in the OpenAI\nconfig for the ",(0,o.jsx)(e.code,{children:"AssistantAgent"}),". Then we register the functions in\n",(0,o.jsx)(e.code,{children:"UserProxyAgent"}),"."]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'class CircumferenceToolInput(BaseModel):\n    radius: float = Field()\n\n\nclass CircumferenceTool(BaseTool):\n    name = "circumference_calculator"\n    description = "Use this tool when you need to calculate a circumference using the radius of a circle"\n    args_schema: Type[BaseModel] = CircumferenceToolInput\n\n    def _run(self, radius: float):\n        return float(radius) * 2.0 * math.pi\n\n\ndef get_file_path_of_example():\n    # Get the current working directory\n    current_dir = os.getcwd()\n\n    # Go one directory up\n    parent_dir = os.path.dirname(current_dir)\n\n    # Move to the target directory\n    target_folder = os.path.join(parent_dir, "test")\n\n    # Construct the path to your target file\n    file_path = os.path.join(target_folder, "test_files/radius.txt")\n\n    return file_path\n'})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# Define a function to generate llm_config from a LangChain tool\n\n\ndef generate_llm_config(tool):\n    # Define the function schema based on the tool\'s args_schema\n    function_schema = {\n        "name": tool.name.lower().replace(" ", "_"),\n        "description": tool.description,\n        "parameters": {\n            "type": "object",\n            "properties": {},\n            "required": [],\n        },\n    }\n\n    if tool.args is not None:\n        function_schema["parameters"]["properties"] = tool.args\n\n    return function_schema\n\n\n# Instantiate the ReadFileTool\nread_file_tool = ReadFileTool()\ncustom_tool = CircumferenceTool()\n\n# Construct the llm_config\nllm_config = {\n    # Generate functions config for the Tool\n    "functions": [\n        generate_llm_config(custom_tool),\n        generate_llm_config(read_file_tool),\n    ],\n    "config_list": config_list,  # Assuming you have this defined elsewhere\n    "timeout": 120,\n}\n\nuser_proxy = autogen.UserProxyAgent(\n    name="user_proxy",\n    is_termination_msg=lambda x: x.get("content", "") and x.get("content", "").rstrip().endswith("TERMINATE"),\n    human_input_mode="NEVER",\n    max_consecutive_auto_reply=10,\n    code_execution_config={\n        "work_dir": "coding",\n        "use_docker": False,\n    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n)\n\n# Register the tool and start the conversation\nuser_proxy.register_function(\n    function_map={\n        custom_tool.name: custom_tool._run,\n        read_file_tool.name: read_file_tool._run,\n    }\n)\n\nchatbot = autogen.AssistantAgent(\n    name="chatbot",\n    system_message="For coding tasks, only use the functions you have been provided with. Reply TERMINATE when the task is done.",\n    llm_config=llm_config,\n)\n\nuser_proxy.initiate_chat(\n    chatbot,\n    message=f"Read the file with the path {get_file_path_of_example()}, then calculate the circumference of a circle that has a radius of that files contents.",  # 7.81mm in the file\n    llm_config=llm_config,\n)\n'})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-text",children:'user_proxy (to chatbot):\n\nRead the file with the path \'Test.txt\', then calculate the circumference of a circle that has a radius of that files contents.\n\n--------------------------------------------------------------------------------\nchatbot (to user_proxy):\n\n***** Suggested function Call: read_file *****\nArguments: \n{\n  "file_path": "Test.txt"\n}\n**********************************************\n\n--------------------------------------------------------------------------------\n\n>>>>>>>> EXECUTING FUNCTION read_file...\nuser_proxy (to chatbot):\n\n***** Response from calling function "read_file" *****\n7.81mm\n******************************************************\n\n--------------------------------------------------------------------------------\nchatbot (to user_proxy):\n\n***** Suggested function Call: circumference_calculator *****\nArguments: \n{\n  "radius": 7.81\n}\n*************************************************************\n\n--------------------------------------------------------------------------------\n\n>>>>>>>> EXECUTING FUNCTION circumference_calculator...\nuser_proxy (to chatbot):\n\n***** Response from calling function "circumference_calculator" *****\n49.071677249072565\n*********************************************************************\n\n--------------------------------------------------------------------------------\nchatbot (to user_proxy):\n\nThe circumference of a circle with a radius of 7.81mm is approximately 49.071mm.\n\n--------------------------------------------------------------------------------\nuser_proxy (to chatbot):\n\n\n\n--------------------------------------------------------------------------------\nchatbot (to user_proxy):\n\nTERMINATE\n\n--------------------------------------------------------------------------------\n'})}),"\n",(0,o.jsx)(e.h1,{id:"a-pyspark-example",children:"A PySpark Example"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"%pip install pyspark\n"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-text",children:"Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\nRequirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'spark = SparkSession.builder.getOrCreate()\nschema = "langchain_example"\nspark.sql(f"CREATE DATABASE IF NOT EXISTS {schema}")\nspark.sql(f"USE {schema}")\ncsv_file_path = "./sample_data/california_housing_train.csv"\ntable = "california_housing_train"\nspark.read.csv(csv_file_path, header=True, inferSchema=True).write.option(\n    "path", "file:/content/spark-warehouse/langchain_example.db/california_housing_train"\n).mode("overwrite").saveAsTable(table)\nspark.table(table).show()\n'})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-text",children:"+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n|  -114.31|   34.19|              15.0|     5612.0|        1283.0|    1015.0|     472.0|       1.4936|           66900.0|\n|  -114.47|    34.4|              19.0|     7650.0|        1901.0|    1129.0|     463.0|         1.82|           80100.0|\n|  -114.56|   33.69|              17.0|      720.0|         174.0|     333.0|     117.0|       1.6509|           85700.0|\n|  -114.57|   33.64|              14.0|     1501.0|         337.0|     515.0|     226.0|       3.1917|           73400.0|\n|  -114.57|   33.57|              20.0|     1454.0|         326.0|     624.0|     262.0|        1.925|           65500.0|\n|  -114.58|   33.63|              29.0|     1387.0|         236.0|     671.0|     239.0|       3.3438|           74000.0|\n|  -114.58|   33.61|              25.0|     2907.0|         680.0|    1841.0|     633.0|       2.6768|           82400.0|\n|  -114.59|   34.83|              41.0|      812.0|         168.0|     375.0|     158.0|       1.7083|           48500.0|\n|  -114.59|   33.61|              34.0|     4789.0|        1175.0|    3134.0|    1056.0|       2.1782|           58400.0|\n|   -114.6|   34.83|              46.0|     1497.0|         309.0|     787.0|     271.0|       2.1908|           48100.0|\n|   -114.6|   33.62|              16.0|     3741.0|         801.0|    2434.0|     824.0|       2.6797|           86500.0|\n|   -114.6|    33.6|              21.0|     1988.0|         483.0|    1182.0|     437.0|        1.625|           62000.0|\n|  -114.61|   34.84|              48.0|     1291.0|         248.0|     580.0|     211.0|       2.1571|           48600.0|\n|  -114.61|   34.83|              31.0|     2478.0|         464.0|    1346.0|     479.0|        3.212|           70400.0|\n|  -114.63|   32.76|              15.0|     1448.0|         378.0|     949.0|     300.0|       0.8585|           45000.0|\n|  -114.65|   34.89|              17.0|     2556.0|         587.0|    1005.0|     401.0|       1.6991|           69100.0|\n|  -114.65|    33.6|              28.0|     1678.0|         322.0|     666.0|     256.0|       2.9653|           94900.0|\n|  -114.65|   32.79|              21.0|       44.0|          33.0|      64.0|      27.0|       0.8571|           25000.0|\n|  -114.66|   32.74|              17.0|     1388.0|         386.0|     775.0|     320.0|       1.2049|           44000.0|\n|  -114.67|   33.92|              17.0|       97.0|          24.0|      29.0|      15.0|       1.2656|           27500.0|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\nonly showing top 20 rows\n"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# Note, you can also connect to Spark via Spark connect. For example:\n# db = SparkSQL.from_uri("sc://localhost:15002", schema=schema)\nspark_sql = SparkSQL(schema=schema)\nllm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo-16k")\ntoolkit = SparkSQLToolkit(db=spark_sql, llm=llm)\nagent_executor = create_spark_sql_agent(llm=llm, toolkit=toolkit, verbose=True)\n'})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# Starndard Langchain example\nagent_executor.run("Describe the california_housing_train table")\n'})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-text",children:'\n\n> Entering new AgentExecutor chain...\nAction: list_tables_sql_db\nAction Input: ""\nObservation: california_housing_train\nThought:I see that there is a table called "california_housing_train" in the database. I can use the "schema_sql_db" tool to get more information about this table.\nAction: schema_sql_db\nAction Input: "california_housing_train"\nObservation: CREATE TABLE spark_catalog.langchain_example.california_housing_train (\n  longitude DOUBLE,\n  latitude DOUBLE,\n  housing_median_age DOUBLE,\n  total_rooms DOUBLE,\n  total_bedrooms DOUBLE,\n  population DOUBLE,\n  households DOUBLE,\n  median_income DOUBLE,\n  median_house_value DOUBLE)\n;\n\n/*\n3 rows from california_housing_train table:\nlongitude   latitude    housing_median_age  total_rooms total_bedrooms  population  households  median_income   median_house_value\n-114.31 34.19   15.0    5612.0  1283.0  1015.0  472.0   1.4936  66900.0\n-114.47 34.4    19.0    7650.0  1901.0  1129.0  463.0   1.82    80100.0\n-114.56 33.69   17.0    720.0   174.0   333.0   117.0   1.6509  85700.0\n*/\nThought:The "california_housing_train" table has the following columns: longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, and median_house_value. It contains information about housing in California. I can now answer the question.\nAction: None\nFinal Answer: The "california_housing_train" table has the following columns: longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, and median_house_value.\n\n> Finished chain.\n'})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-text",children:"'The \"california_housing_train\" table has the following columns: longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, and median_house_value.'\n"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# LangChain direct tool usage instead of toolkit example\n# from langchain.tools.spark_sql.tool import (\n#     InfoSparkSQLTool,\n#     ListSparkSQLTool,\n#     QueryCheckerTool,\n#     QuerySparkSQLTool,\n# )\n# debug_toolkit = [\n#   QuerySparkSQLTool(db=spark_sql),\n#   InfoSparkSQLTool(db=spark_sql),\n#   ListSparkSQLTool(db=spark_sql),\n#   QueryCheckerTool(db=spark_sql, llm=llm),\n# ]\n"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# Now use AutoGen with Langchain Tool Bridgre\ntools = []\nfunction_map = {}\n\nfor tool in toolkit.get_tools():  # debug_toolkit if you want to use tools directly\n    tool_schema = generate_llm_config(tool)\n    print(tool_schema)\n    tools.append(tool_schema)\n    function_map[tool.name] = tool._run\n\n# Construct the llm_config\nllm_config = {\n    "functions": tools,\n    "config_list": config_list,  # Assuming you have this defined elsewhere\n    "timeout": 120,\n}\n\nuser_proxy = autogen.UserProxyAgent(\n    name="user_proxy",\n    is_termination_msg=lambda x: x.get("content", "") and x.get("content", "").rstrip().endswith("TERMINATE"),\n    human_input_mode="NEVER",\n    max_consecutive_auto_reply=10,\n    code_execution_config={\n        "work_dir": "coding",\n        "use_docker": False,\n    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n)\n\nprint(function_map)\n\n# Register the tool and start the conversation\nuser_proxy.register_function(function_map=function_map)\n\nchatbot = autogen.AssistantAgent(\n    name="chatbot",\n    system_message="For coding tasks, only use the functions you have been provided with. Reply TERMINATE when the task is done.",\n    llm_config=llm_config,\n)\n\nuser_proxy.initiate_chat(\n    chatbot,\n    message="Describe the table names california_housing_train",\n    llm_config=llm_config,\n)\n'})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-text",children:"{'name': 'query_sql_db', 'description': '\\n    Input to this tool is a detailed and correct SQL query, output is a result from the Spark SQL.\\n    If the query is not correct, an error message will be returned.\\n    If an error is returned, rewrite the query, check the query, and try again.\\n    ', 'parameters': {'type': 'object', 'properties': {'query': {'title': 'Query', 'type': 'string'}}, 'required': []}}\n{'name': 'schema_sql_db', 'description': '\\n    Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables.\\n    Be sure that the tables actually exist by calling list_tables_sql_db first!\\n\\n    Example Input: \"table1, table2, table3\"\\n    ', 'parameters': {'type': 'object', 'properties': {'table_names': {'title': 'Table Names', 'type': 'string'}}, 'required': []}}\n{'name': 'list_tables_sql_db', 'description': 'Input is an empty string, output is a comma separated list of tables in the Spark SQL.', 'parameters': {'type': 'object', 'properties': {'tool_input': {'title': 'Tool Input', 'default': '', 'type': 'string'}}, 'required': []}}\n{'name': 'query_checker_sql_db', 'description': '\\n    Use this tool to double check if your query is correct before executing it.\\n    Always use this tool before executing a query with query_sql_db!\\n    ', 'parameters': {'type': 'object', 'properties': {'query': {'title': 'Query', 'type': 'string'}}, 'required': []}}\nuser_proxy (to chatbot):\n\nDescribe the table names california_housing_train\n\n--------------------------------------------------------------------------------\nchatbot (to user_proxy):\n\n***** Suggested function Call: schema_sql_db *****\nArguments: \n{\n  \"table_names\": \"california_housing_train\"\n}\n**************************************************\n\n--------------------------------------------------------------------------------\n\n>>>>>>>> EXECUTING FUNCTION schema_sql_db...\nuser_proxy (to chatbot):\n\n***** Response from calling function \"schema_sql_db\" *****\nCREATE TABLE spark_catalog.langchain_example.california_housing_train (\n  longitude DOUBLE,\n  latitude DOUBLE,\n  housing_median_age DOUBLE,\n  total_rooms DOUBLE,\n  total_bedrooms DOUBLE,\n  population DOUBLE,\n  households DOUBLE,\n  median_income DOUBLE,\n  median_house_value DOUBLE)\n;\n\n/*\n3 rows from california_housing_train table:\nlongitude   latitude    housing_median_age  total_rooms total_bedrooms  population  households  median_income   median_house_value\n-114.31 34.19   15.0    5612.0  1283.0  1015.0  472.0   1.4936  66900.0\n-114.47 34.4    19.0    7650.0  1901.0  1129.0  463.0   1.82    80100.0\n-114.56 33.69   17.0    720.0   174.0   333.0   117.0   1.6509  85700.0\n*/\n**********************************************************\n\n--------------------------------------------------------------------------------\nchatbot (to user_proxy):\n\nThe california_housing_train table has the following schema:\n\n- longitude: DOUBLE\n- latitude: DOUBLE\n- housing_median_age: DOUBLE\n- total_rooms: DOUBLE\n- total_bedrooms: DOUBLE\n- population: DOUBLE\n- households: DOUBLE\n- median_income: DOUBLE\n- median_house_value: DOUBLE\n\nHere are sample rows from the table:\n\n| longitude | latitude | housing_median_age | total_rooms | total_bedrooms | population | households | median_income | median_house_value |\n|-----------|----------|--------------------|-------------|----------------|------------|------------|---------------|--------------------|\n| -114.31   | 34.19    | 15.0               | 5612.0      | 1283.0         | 1015.0     | 472.0      | 1.4936        | 66900.0            |\n| -114.47   | 34.4     | 19.0               | 7650.0      | 1901.0         | 1129.0     | 463.0      | 1.82          | 80100.0            |\n| -114.56   | 33.69    | 17.0               | 720.0       | 174.0          | 333.0      | 117.0      | 1.6509        | 85700.0            |\n\n--------------------------------------------------------------------------------\nuser_proxy (to chatbot):\n\n\n\n--------------------------------------------------------------------------------\nchatbot (to user_proxy):\n\nTERMINATE\n\n--------------------------------------------------------------------------------\n"})})]})}function u(n={}){const{wrapper:e}={...(0,a.a)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(h,{...n})}):h(n)}},11151:(n,e,t)=>{t.d(e,{Z:()=>r,a:()=>s});var o=t(67294);const a={},i=o.createContext(a);function s(n){const e=o.useContext(i);return o.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:s(n.components),o.createElement(i.Provider,{value:e},n.children)}}}]);