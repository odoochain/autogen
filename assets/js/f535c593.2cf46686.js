"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[4234],{10891:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>o,metadata:()=>r,toc:()=>c});var a=t(85893),i=t(11151);const o={custom_edit_url:"https://github.com/autogen-ai/autogen/edit/main/notebook/autogen_uniformed_api_calling.ipynb",description:"Uniform interface to call different LLM.",source_notebook:"/notebook/autogen_uniformed_api_calling.ipynb",tags:["integration","custom model"],title:"A Uniform interface to call different LLMs"},s="A Uniform interface to call different LLMs",r={id:"notebooks/autogen_uniformed_api_calling",title:"A Uniform interface to call different LLMs",description:"Uniform interface to call different LLM.",source:"@site/docs/notebooks/autogen_uniformed_api_calling.mdx",sourceDirName:"notebooks",slug:"/notebooks/autogen_uniformed_api_calling",permalink:"/autogen/docs/notebooks/autogen_uniformed_api_calling",draft:!1,unlisted:!1,editUrl:"https://github.com/autogen-ai/autogen/edit/main/notebook/autogen_uniformed_api_calling.ipynb",tags:[{label:"integration",permalink:"/autogen/docs/tags/integration"},{label:"custom model",permalink:"/autogen/docs/tags/custom-model"}],version:"current",frontMatter:{custom_edit_url:"https://github.com/autogen-ai/autogen/edit/main/notebook/autogen_uniformed_api_calling.ipynb",description:"Uniform interface to call different LLM.",source_notebook:"/notebook/autogen_uniformed_api_calling.ipynb",tags:["integration","custom model"],title:"A Uniform interface to call different LLMs"},sidebar:"notebooksSidebar",previous:{title:"AutoBuild",permalink:"/autogen/docs/notebooks/autobuild_basic"},next:{title:"From Dad Jokes To Sad Jokes: Function Calling with GPTAssistantAgent",permalink:"/autogen/docs/notebooks/gpt_assistant_agent_function_call"}},l={},c=[{value:"Install required packages",id:"install-required-packages",level:2},{value:"Config list setup",id:"config-list-setup",level:2},{value:"Uniform Interface to call different LLMs",id:"uniform-interface-to-call-different-llms",level:2},{value:"Using different LLMs in agents",id:"using-different-llms-in-agents",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",img:"img",p:"p",pre:"pre",...(0,i.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"a-uniform-interface-to-call-different-llms",children:"A Uniform interface to call different LLMs"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://colab.research.google.com/github/autogen-ai/autogen/blob/main/notebook/autogen_uniformed_api_calling.ipynb",children:(0,a.jsx)(n.img,{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})}),"\n",(0,a.jsx)(n.a,{href:"https://github.com/autogen-ai/autogen/blob/main/notebook/autogen_uniformed_api_calling.ipynb",children:(0,a.jsx)(n.img,{src:"https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github",alt:"Open on GitHub"})})]}),"\n",(0,a.jsxs)(n.p,{children:["Autogen provides a uniform interface for API calls to different LLMs,\nand creating LLM agents from them. Through setting up a configuration\nfile, you can easily switch between different LLMs by just changing the\nmodel name, while enjoying all the ",(0,a.jsx)(n.a,{href:"https://autogen-ai.github.io/autogen/docs/topics/llm-caching",children:"enhanced\nfeatures"}),"\nsuch as\n",(0,a.jsx)(n.a,{href:"https://autogen-ai.github.io/autogen/docs/Use-Cases/enhanced_inference/#usage-summary",children:"caching"}),"\nand ",(0,a.jsx)(n.a,{href:"https://autogen-ai.github.io/autogen/docs/Use-Cases/enhanced_inference/#usage-summary",children:"cost\ncalculation"}),"!"]}),"\n",(0,a.jsx)(n.p,{children:"In this notebook, we will show you how to use AutoGen to call different\nLLMs and create LLM agents from them."}),"\n",(0,a.jsxs)(n.p,{children:["Currently, we support the following model families: -\n",(0,a.jsx)(n.a,{href:"https://platform.openai.com/docs/overview",children:"OpenAI"})," - ",(0,a.jsx)(n.a,{href:"https://azure.microsoft.com/en-us/products/ai-services/openai-service/?ef_id=_k_CjwKCAjwps-zBhAiEiwALwsVYdbpVkqA3IbY7WnxtrjNSefBnTfrijwRAFaYd8uuLCjeWsPdfZmxUBoC_ZAQAvD_BwE_k_&OCID=AIDcmm5edswduu_SEM__k_CjwKCAjwps-zBhAiEiwALwsVYdbpVkqA3IbY7WnxtrjNSefBnTfrijwRAFaYd8uuLCjeWsPdfZmxUBoC_ZAQAvD_BwE_k_&gad_source=1&gclid=CjwKCAjwps-zBhAiEiwALwsVYdbpVkqA3IbY7WnxtrjNSefBnTfrijwRAFaYd8uuLCjeWsPdfZmxUBoC_ZAQAvD_BwE",children:"Azure\nOpenAI"})," -\n",(0,a.jsx)(n.a,{href:"https://docs.anthropic.com/en/docs/welcome",children:"Anthropic Claude"})," - ",(0,a.jsx)(n.a,{href:"https://ai.google.dev/gemini-api/docs",children:"Google\nGemini"})," -\n",(0,a.jsx)(n.a,{href:"https://docs.mistral.ai/",children:"Mistral"})," (API to open and closed-source\nmodels) - ",(0,a.jsx)(n.a,{href:"https://deepinfra.com/",children:"DeepInfra"})," (API to open-source\nmodels) - ",(0,a.jsx)(n.a,{href:"https://www.together.ai/",children:"TogetherAI"})," (API to open-source\nmodels)"]}),"\n",(0,a.jsx)(n.p,{children:"\u2026 and more to come!"}),"\n",(0,a.jsxs)(n.p,{children:["You can also ",(0,a.jsx)(n.a,{href:"https://autogen-ai.github.io/autogen/blog/2024/01/26/Custom-Models",children:"plug in your local deployed\nLLM"}),"\ninto AutoGen if needed."]}),"\n",(0,a.jsx)(n.h2,{id:"install-required-packages",children:"Install required packages"}),"\n",(0,a.jsx)(n.p,{children:"You may want to install AutoGen with options to different LLMs. Here we\ninstall AutoGen with all the supported LLMs. By default, AutoGen is\ninstalled with OpenAI support."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip install pyautogen[gemini,anthropic,mistral,together]\n"})}),"\n",(0,a.jsx)(n.h2,{id:"config-list-setup",children:"Config list setup"}),"\n",(0,a.jsxs)(n.p,{children:["First, create a ",(0,a.jsx)(n.code,{children:"OAI_CONFIG_LIST"})," file to specify the api keys for the\nLLMs you want to use. Generally, you just need to specify the ",(0,a.jsx)(n.code,{children:"model"}),",\n",(0,a.jsx)(n.code,{children:"api_key"})," and ",(0,a.jsx)(n.code,{children:"api_type"})," from the provider."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'[\n    {   \n        # using OpenAI\n        "model": "gpt-35-turbo-1106", \n        "api_key": "YOUR_API_KEY"\n        # default api_type is openai\n    },\n    {\n        # using Azure OpenAI\n        "model": "gpt-4-turbo-1106",\n        "api_key": "YOUR_API_KEY",\n        "api_type": "azure",\n        "base_url": "YOUR_BASE_URL",\n        "api_version": "YOUR_API_VERSION"\n    },\n    {   \n        # using Google gemini\n        "model": "gemini-1.5-pro-latest",\n        "api_key": "YOUR_API_KEY",\n        "api_type": "google"\n    },\n    {\n        # using DeepInfra\n        "model": "meta-llama/Meta-Llama-3-70B-Instruct",\n        "api_key": "YOUR_API_KEY",\n        "base_url": "https://api.deepinfra.com/v1/openai" # need to specify the base_url\n    },\n    {\n        # using Anthropic Claude\n        "model": "claude-1.0",\n        "api_type": "anthropic",\n        "api_key": "YOUR_API_KEY"\n    },\n    {\n        # using Mistral\n        "model": "mistral-large-latest",\n        "api_type": "mistral",\n        "api_key": "YOUR_API_KEY"\n    },\n    {\n        # using TogetherAI\n        "model": "google/gemma-7b-it",\n        "api_key": "YOUR_API_KEY",\n        "api_type": "together"\n    }\n    ...\n]\n'})}),"\n",(0,a.jsx)(n.h2,{id:"uniform-interface-to-call-different-llms",children:"Uniform Interface to call different LLMs"}),"\n",(0,a.jsx)(n.p,{children:"We first demonstrate how to use AutoGen to call different LLMs with the\nsame wrapper class."}),"\n",(0,a.jsxs)(n.p,{children:["After you install relevant packages and setup your config list, you only\nneed three steps to call different LLMs: 1. Extract the config with the\nmodel name you want to use. 2. create a client with the model name. 3.\ncall the client ",(0,a.jsx)(n.code,{children:"create"})," to get the response."]}),"\n",(0,a.jsxs)(n.p,{children:["Below, we define a helper function ",(0,a.jsx)(n.code,{children:"model_call_example_function"})," to\nimplement the above steps."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import autogen\nfrom autogen import OpenAIWrapper\n\n\ndef model_call_example_function(model: str, message: str, cache_seed: int = 41, print_cost: bool = False):\n    """\n    A helper function that demonstrates how to call different models using the OpenAIWrapper class.\n    Note the name `OpenAIWrapper` is not accurate, as now it is a wrapper for multiple models, not just OpenAI.\n    This might be changed in the future.\n    """\n    config_list = autogen.config_list_from_json(\n        "OAI_CONFIG_LIST",\n        filter_dict={\n            "model": [model],\n        },\n    )\n    client = OpenAIWrapper(config_list=config_list)\n    response = client.create(messages=[{"role": "user", "content": message}], cache_seed=cache_seed)\n\n    print(f"Response from model {model}: {response.choices[0].message.content}")\n\n    # Print the cost of the API call\n    if print_cost:\n        client.print_usage_summary()\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'model_call_example_function(model="gpt-35-turbo-1106", message="Tell me a joke.")\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"Response from model gpt-35-turbo-1106: Why couldn't the bicycle stand up by itself?\n\nBecause it was two-tired!\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'model_call_example_function(model="gemini-1.5-pro-latest", message="Tell me a joke.")\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"Response from model gemini-1.5-pro-latest: Why don't scientists trust atoms? \n\nBecause they make up everything! \n \nLet me know if you'd like to hear another one! \n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'model_call_example_function(model="meta-llama/Meta-Llama-3-70B-Instruct", message="Tell me a joke. ")\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"Response from model meta-llama/Meta-Llama-3-70B-Instruct: Here's one:\n\nWhy couldn't the bicycle stand up by itself?\n\n(wait for it...)\n\nBecause it was two-tired!\n\nHow was that? Do you want to hear another one?\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'model_call_example_function(model="mistral-large-latest", message="Tell me a joke. ", print_cost=True)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"Response from model mistral-large-latest: Sure, here's a light-hearted joke for you:\n\nWhy don't scientists trust atoms?\n\nBecause they make up everything!\n----------------------------------------------------------------------------------------------------\nUsage summary excluding cached usage: \nTotal cost: 0.00042\n* Model 'mistral-large-latest': cost: 0.00042, prompt_tokens: 9, completion_tokens: 32, total_tokens: 41\n\nAll completions are non-cached: the total cost with cached completions is the same as actual cost.\n----------------------------------------------------------------------------------------------------\n"})}),"\n",(0,a.jsx)(n.h2,{id:"using-different-llms-in-agents",children:"Using different LLMs in agents"}),"\n",(0,a.jsx)(n.p,{children:"Below we give a quick demo of using different LLMs agents in a\ngroupchat."}),"\n",(0,a.jsx)(n.p,{children:"We mock a debate scenario where each LLM agent is a debater, either in\naffirmative or negative side. We use a round-robin strategy to let each\ndebater from different teams to speak in turn."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def get_llm_config(model_name):\n    return {\n        "config_list": autogen.config_list_from_json("OAI_CONFIG_LIST", filter_dict={"model": [model_name]}),\n        "cache_seed": 41,\n    }\n\n\naffirmative_system_message = "You are in the Affirmative team of a debate. When it is your turn, please give at least one reason why you are for the topic. Keep it short."\nnegative_system_message = "You are in the Negative team of a debate. The affirmative team has given their reason, please counter their argument. Keep it short."\n\ngpt35_agent = autogen.AssistantAgent(\n    name="GPT35", system_message=affirmative_system_message, llm_config=get_llm_config("gpt-35-turbo-1106")\n)\n\nllama_agent = autogen.AssistantAgent(\n    name="Llama3",\n    system_message=negative_system_message,\n    llm_config=get_llm_config("meta-llama/Meta-Llama-3-70B-Instruct"),\n)\n\nmistral_agent = autogen.AssistantAgent(\n    name="Mistral", system_message=affirmative_system_message, llm_config=get_llm_config("mistral-large-latest")\n)\n\ngemini_agent = autogen.AssistantAgent(\n    name="Gemini", system_message=negative_system_message, llm_config=get_llm_config("gemini-1.5-pro-latest")\n)\n\nclaude_agent = autogen.AssistantAgent(\n    name="Claude", system_message=affirmative_system_message, llm_config=get_llm_config("claude-3-opus-20240229")\n)\n\nuser_proxy = autogen.UserProxyAgent(\n    name="User",\n    code_execution_config=False,\n)\n\n# initilize the groupchat with round robin speaker selection method\ngroupchat = autogen.GroupChat(\n    agents=[claude_agent, gemini_agent, mistral_agent, llama_agent, gpt35_agent, user_proxy],\n    messages=[],\n    max_round=8,\n    speaker_selection_method="round_robin",\n)\nmanager = autogen.GroupChatManager(groupchat=groupchat)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'chat_history = user_proxy.initiate_chat(recipient=manager, message="Debate Topic: Should vaccination be mandatory?")\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"User (to chat_manager):\n\nDebate Topic: Should vaccination be mandatory?\n\n--------------------------------------------------------------------------------\n\nNext speaker: Claude\n\nClaude (to chat_manager):\n\nAs a member of the Affirmative team, I believe that vaccination should be mandatory for several reasons:\n\n1. Herd immunity: When a large percentage of the population is vaccinated, it helps protect those who cannot receive vaccines due to medical reasons or weakened immune systems. Mandatory vaccination ensures that we maintain a high level of herd immunity, preventing the spread of dangerous diseases.\n\n2. Public health: Vaccines have been proven to be safe and effective in preventing the spread of infectious diseases. By making vaccination mandatory, we prioritize public health and reduce the risk of outbreaks that could lead to widespread illness and loss of life.\n\n3. Societal benefits: Mandatory vaccination not only protects individuals but also benefits society as a whole. It reduces healthcare costs associated with treating preventable diseases and minimizes the economic impact of disease outbreaks on businesses and communities.\n\nIn summary, mandatory vaccination is a critical tool in protecting public health, maintaining herd immunity, and promoting the well-being of our society.\n\n--------------------------------------------------------------------------------\n\nNext speaker: Gemini\n\nGemini (to chat_manager):\n\nWhile we acknowledge the importance of herd immunity and public health,  mandating vaccinations infringes upon individual autonomy and medical freedom.  Blanket mandates fail to consider individual health circumstances and potential vaccine risks, which are often overlooked in favor of a one-size-fits-all approach. \n\n\n--------------------------------------------------------------------------------\n\nNext speaker: Mistral\n\nMistral (to chat_manager):\n\nI understand your concerns and the value of individual autonomy. However, it's important to note that mandatory vaccination policies often include exemptions for medical reasons. This allows for individual health circumstances to be taken into account, ensuring that those who cannot safely receive vaccines are not put at risk. The goal is to strike a balance between protecting public health and respecting individual choices, while always prioritizing the well-being and safety of all members of society.\n\n--------------------------------------------------------------------------------\n\nNext speaker: Llama3\n\nLlama3 (to chat_manager):\n\nI understand your point, but blanket exemptions for medical reasons are not sufficient to address the complexities of individual health circumstances. What about those who have experienced adverse reactions to vaccines in the past or have a family history of such reactions? What about those who have compromised immune systems or are taking medications that may interact with vaccine components? A one-size-fits-all approach to vaccination ignores the nuances of individual health and puts some people at risk of harm. Additionally, mandating vaccination undermines trust in government and healthcare institutions, leading to further divides and mistrust. We need to prioritize informed consent and individual autonomy in medical decisions, rather than relying solely on a blanket mandate.\n\n--------------------------------------------------------------------------------\n\nNext speaker: GPT35\n\nGPT35 (to chat_manager):\n\nI understand your point, but mandatory vaccination policies can still allow for exemptions based on medical history, allergic reactions, and compromised immunity. This would address the individual circumstances you mentioned. Furthermore, mandating vaccination can also help strengthen trust in public health measures by demonstrating a commitment to protecting the entire community. Informed consent is important, but it is also essential to consider the potential consequences of not being vaccinated on public health and the well-being of others.\n\n--------------------------------------------------------------------------------\n\nNext speaker: User\n"})})]})}function m(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>r,a:()=>s});var a=t(67294);const i={},o=a.createContext(i);function s(e){const n=a.useContext(o);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);