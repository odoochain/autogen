"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[6989],{57741:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>g,frontMatter:()=>o,metadata:()=>a,toc:()=>c});var s=t(85893),i=t(11151);const o={custom_edit_url:"https://github.com/autogen-ai/autogen/edit/main/notebook/lats_search.ipynb",description:"Language Agent Tree Search.",source_notebook:"/notebook/lats_search.ipynb",tags:["LATS","search","reasoning","reflection"],title:"Language Agent Tree Search"},r="Language Agent Tree Search",a={id:"notebooks/lats_search",title:"Language Agent Tree Search",description:"Language Agent Tree Search.",source:"@site/docs/notebooks/lats_search.mdx",sourceDirName:"notebooks",slug:"/notebooks/lats_search",permalink:"/autogen/docs/notebooks/lats_search",draft:!1,unlisted:!1,editUrl:"https://github.com/autogen-ai/autogen/edit/main/notebook/lats_search.ipynb",tags:[{label:"LATS",permalink:"/autogen/docs/tags/lats"},{label:"search",permalink:"/autogen/docs/tags/search"},{label:"reasoning",permalink:"/autogen/docs/tags/reasoning"},{label:"reflection",permalink:"/autogen/docs/tags/reflection"}],version:"current",frontMatter:{custom_edit_url:"https://github.com/autogen-ai/autogen/edit/main/notebook/lats_search.ipynb",description:"Language Agent Tree Search.",source_notebook:"/notebook/lats_search.ipynb",tags:["LATS","search","reasoning","reflection"],title:"Language Agent Tree Search"},sidebar:"notebooksSidebar",previous:{title:"From Dad Jokes To Sad Jokes: Function Calling with GPTAssistantAgent",permalink:"/autogen/docs/notebooks/gpt_assistant_agent_function_call"}},l={},c=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Reflection Class",id:"reflection-class",level:3},{value:"Tree State",id:"tree-state",level:2},{value:"Define Language Agent",id:"define-language-agent",level:2},{value:"Tools",id:"tools",level:4},{value:"Reflection",id:"reflection",level:3},{value:"Initial Response",id:"initial-response",level:3},{value:"Starting Node",id:"starting-node",level:4},{value:"Candidate Generation",id:"candidate-generation",level:3},{value:"Candidate generation node",id:"candidate-generation-node",level:4},{value:"Create Tree",id:"create-tree",level:2},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",...(0,i.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"language-agent-tree-search",children:"Language Agent Tree Search"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://colab.research.google.com/github/autogen-ai/autogen/blob/main/notebook/lats_search.ipynb",children:(0,s.jsx)(n.img,{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})}),"\n",(0,s.jsx)(n.a,{href:"https://github.com/autogen-ai/autogen/blob/main/notebook/lats_search.ipynb",children:(0,s.jsx)(n.img,{src:"https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github",alt:"Open on GitHub"})})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2310.04406",children:"Language Agent Tree Search"})," (LATS),\nby Zhou, et. al, is a general LLM agent search algorithm that combines\nreflection/evaluation and search (specifically Monte-Carlo tree search)\nto achieve stronger overall task performance by leveraging\ninference-time compute."]}),"\n",(0,s.jsx)(n.p,{children:"It has four main phases consisting of six steps:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Select: pick the best next state to progress from, based on its\naggregate value."}),"\n",(0,s.jsx)(n.li,{children:"Expand and simulate: sample n potential actions to take and execute\nthem in parallel."}),"\n",(0,s.jsx)(n.li,{children:"Reflect + Evaluate: observe the outcomes of these actions and score\nthe decisions based on reflection (and possibly external feedback if\navailable)"}),"\n",(0,s.jsx)(n.li,{children:"Backpropagate: update the scores of the root trajectories based on\nthe outcomes."}),"\n"]}),"\n",(0,s.jsxs)("figure",{children:[(0,s.jsx)("img",{src:"https://i.postimg.cc/NjQScLTv/image.png",alt:"lats"}),(0,s.jsx)("figcaption",{"aria-hidden":"true",children:"lats"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import json\nimport logging\nimport os\nimport uuid\nfrom typing import Any, Dict, List\n\nfrom autogen import AssistantAgent, ConversableAgent, GroupChat, UserProxyAgent, config_list_from_json\n"})}),"\n",(0,s.jsx)(n.h1,{id:"configure-logging",children:"Configure logging"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"logging.basicConfig(level=logging.INFO)\n"})}),"\n",(0,s.jsx)(n.h1,{id:"set-environment-variables",children:"Set environment variables"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'os.environ["AUTOGEN_USE_DOCKER"] = "0"  # Disable Docker usage globally for Autogen\nos.environ["OPENAI_API_KEY"] = "YOUR_API_KEY"\n'})}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.p,{children:["Install ",(0,s.jsx)(n.code,{children:"autogen"})," (for the LLM framework and agents)"]}),"\n",(0,s.jsx)(n.p,{children:"Required packages: autogen"}),"\n",(0,s.jsx)(n.p,{children:"Please ensure these packages are installed before running this script"}),"\n",(0,s.jsx)(n.h1,{id:"directly-create-the-config_list-with-the-api-key",children:"Directly create the config_list with the API key"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'config_list = [{"model": "gpt-4o-mini", "api_key": "YOUR_API_KEY"}]\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'if not config_list:\n    raise ValueError("Failed to create configuration. Please check the API key.")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"reflection-class",children:"Reflection Class"}),"\n",(0,s.jsx)(n.p,{children:"The reflection chain will score agent outputs based on the decision and\nthe tool responses."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from pydantic import BaseModel, Field\n\n\nclass Reflection(BaseModel):\n    reflections: str = Field(\n        description="The critique and reflections on the sufficiency, superfluency,"\n        " and general quality of the response"\n    )\n    score: int = Field(\n        description="Score from 0-10 on the quality of the candidate response.",\n        gte=0,\n        lte=10,\n    )\n    found_solution: bool = Field(description="Whether the response has fully solved the question or task.")\n\n    def as_message(self):\n        return {"role": "human", "content": f"Reasoning: {self.reflections}\\nScore: {self.score}"}\n\n    @property\n    def normalized_score(self) -> float:\n        return self.score / 10.0\n'})}),"\n",(0,s.jsx)(n.h2,{id:"tree-state",children:"Tree State"}),"\n",(0,s.jsx)(n.p,{children:"LATS is based on a (greedy) Monte-Carlo tree search. For each search\nsteps, it picks the node with the highest \u201cupper confidence bound\u201d,\nwhich is a metric that balances exploitation (highest average reward)\nand exploration (lowest visits). Starting from that node, it generates N\n(5 in this case) new candidate actions to take, and adds them to the\ntree. It stops searching either when it has generated a valid solution\nOR when it has reached the maximum number of rollouts (search tree\ndepth)."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import math\nimport os\nfrom collections import deque\nfrom typing import Optional\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class Node:\n    def __init__(\n        self,\n        messages: List[Dict[str, str]],\n        reflection: Optional[Reflection] = None,\n        parent: Optional["Node"] = None,\n    ):\n        self.messages = messages\n        self.parent = parent\n        self.children: List["Node"] = []\n        self.value = 0.0\n        self.visits = 0\n        self.reflection = reflection\n        self.depth = parent.depth + 1 if parent is not None else 1\n        self._is_solved = reflection.found_solution if reflection else False\n        if self._is_solved:\n            self._mark_tree_as_solved()\n        if reflection:\n            self.backpropagate(reflection.normalized_score)\n\n    def __repr__(self) -> str:\n        return (\n            f"<Node value={self.value:.2f}, visits={self.visits}," f" depth={self.depth}, is_solved={self._is_solved}>"\n        )\n\n    @property\n    def is_solved(self) -> bool:\n        """If any solutions exist, we can end the search."""\n        return self._is_solved\n\n    @property\n    def is_terminal(self):\n        return not self.children\n\n    @property\n    def best_child(self):\n        """Select the child with the highest UCT to search next."""\n        if not self.children:\n            return None\n        all_nodes = self._get_all_children()\n        return max(all_nodes, key=lambda child: child.upper_confidence_bound())\n\n    @property\n    def best_child_score(self):\n        """Return the child with the highest value."""\n        if not self.children:\n            return None\n        return max(self.children, key=lambda child: int(child.is_solved) * child.value)\n\n    @property\n    def height(self) -> int:\n        """Check for how far we\'ve rolled out the tree."""\n        if self.children:\n            return 1 + max([child.height for child in self.children])\n        return 1\n\n    def upper_confidence_bound(self, exploration_weight=1.0):\n        """Return the UCT score. This helps balance exploration vs. exploitation of a branch."""\n        if self.parent is None:\n            raise ValueError("Cannot obtain UCT from root node")\n        if self.visits == 0:\n            return self.value\n        # Encourages exploitation of high-value trajectories\n        average_reward = self.value / self.visits\n        exploration_term = math.sqrt(math.log(self.parent.visits) / self.visits)\n        return average_reward + exploration_weight * exploration_term\n\n    def backpropagate(self, reward: float):\n        """Update the score of this node and its parents."""\n        node = self\n        while node:\n            node.visits += 1\n            node.value = (node.value * (node.visits - 1) + reward) / node.visits\n            node = node.parent\n\n    def get_messages(self, include_reflections: bool = True):\n        if include_reflections and self.reflection:\n            return self.messages + [self.reflection.as_message()]\n        return self.messages\n\n    def get_trajectory(self, include_reflections: bool = True) -> List[Dict[str, str]]:\n        """Get messages representing this search branch."""\n        messages = []\n        node = self\n        while node:\n            messages.extend(node.get_messages(include_reflections=include_reflections)[::-1])\n            node = node.parent\n        # Reverse the final back-tracked trajectory to return in the correct order\n        return messages[::-1]  # root solution, reflection, child 1, ...\n\n    def _get_all_children(self):\n        all_nodes = []\n        nodes = deque()\n        nodes.append(self)\n        while nodes:\n            node = nodes.popleft()\n            all_nodes.extend(node.children)\n            for n in node.children:\n                nodes.append(n)\n        return all_nodes\n\n    def get_best_solution(self):\n        """Return the best solution from within the current sub-tree."""\n        all_nodes = [self] + self._get_all_children()\n        best_node = max(\n            all_nodes,\n            # We filter out all non-terminal, non-solution trajectories\n            key=lambda node: int(node.is_terminal and node.is_solved) * node.value,\n        )\n        return best_node\n\n    def _mark_tree_as_solved(self):\n        parent = self.parent\n        while parent:\n            parent._is_solved = True\n            parent = parent.parent\n'})}),"\n",(0,s.jsx)(n.p,{children:"The main component is the tree, represented by the root node."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from typing_extensions import TypedDict\n\n\nclass TreeState(TypedDict):\n    # The full tree\n    root: Node\n    # The original input\n    input: str\n"})}),"\n",(0,s.jsx)(n.h2,{id:"define-language-agent",children:"Define Language Agent"}),"\n",(0,s.jsx)(n.p,{children:"Our agent will have three primary LLM-powered processes:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Reflect: score the action based on the tool response."}),"\n",(0,s.jsx)(n.li,{children:"Initial response: to create the root node and start the search."}),"\n",(0,s.jsx)(n.li,{children:"Expand: generate 5 candidate \u201cnext steps\u201d from the best spot in the\ncurrent tree"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"For more \u201cGrounded\u201d tool applications (such as code synthesis), you\ncould integrate code execution into the reflection/reward step. This\ntype of external feedback is very useful."}),"\n",(0,s.jsx)(n.h4,{id:"tools",children:"Tools"}),"\n",(0,s.jsx)(n.p,{children:"For our example, we will give the language agent a search engine."}),"\n",(0,s.jsx)(n.p,{children:"Define the UserProxyAgent with web search / tool-use capability"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'user_proxy = UserProxyAgent(\n    name="user",\n    human_input_mode="NEVER",\n    max_consecutive_auto_reply=10,\n    code_execution_config={\n        "work_dir": "web",\n        "use_docker": False,\n    },\n)\n'})}),"\n",(0,s.jsx)(n.p,{children:"Create a ConversableAgent without tools"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'assistant_agent = ConversableAgent(\n    name="assistant_agent",\n    system_message="You are an AI assistant capable of helping with various tasks.",\n    human_input_mode="NEVER",\n    code_execution_config=False,\n)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"reflection",children:"Reflection"}),"\n",(0,s.jsx)(n.p,{children:"Self-reflection allows the agent to boostrap, improving its future\nresponses based on the outcome of previous ones. In agents this is more\npowerful since it can use external feedback to improve."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'reflection_prompt = """\nReflect and grade the assistant response to the user question below.\nUser question: {input}\nAssistant response: {candidate}\n\nProvide your reflection in the following format:\nReflections: [Your detailed critique and reflections]\nScore: [A score from 0-10]\nFound Solution: [true/false]\n"""\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'reflection_agent = AssistantAgent(\n    name="reflection_agent",\n    system_message="You are an AI assistant that reflects on and grades responses.",\n    llm_config={\n        "config_list": config_list,\n        "temperature": 0.2,\n    },\n)\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def reflection_chain(inputs: Dict[str, Any]) -> Reflection:\n    try:\n        candidate_content = ""\n        if "candidate" in inputs:\n            candidate = inputs["candidate"]\n            if isinstance(candidate, list):\n                candidate_content = (\n                    candidate[-1]["content"]\n                    if isinstance(candidate[-1], dict) and "content" in candidate[-1]\n                    else str(candidate[-1])\n                )\n            elif isinstance(candidate, dict):\n                candidate_content = candidate.get("content", str(candidate))\n            elif isinstance(candidate, str):\n                candidate_content = candidate\n            else:\n                candidate_content = str(candidate)\n\n        formatted_prompt = [\n            {"role": "system", "content": "You are an AI assistant that reflects on and grades responses."},\n            {\n                "role": "user",\n                "content": reflection_prompt.format(input=inputs.get("input", ""), candidate=candidate_content),\n            },\n        ]\n        response = reflection_agent.generate_reply(formatted_prompt)\n\n        # Parse the response\n        response_str = str(response)\n        lines = response_str.split("\\n")\n        reflections = next((line.split(": ", 1)[1] for line in lines if line.startswith("Reflections:")), "")\n        score_str = next((line.split(": ", 1)[1] for line in lines if line.startswith("Score:")), "0")\n        try:\n            if "/" in score_str:\n                numerator, denominator = map(int, score_str.split("/"))\n                score = int((numerator / denominator) * 10)\n            else:\n                score = int(score_str)\n        except ValueError:\n            logging.warning(f"Invalid score value: {score_str}. Defaulting to 0.")\n        score = 0\n\n        found_solution = next(\n            (line.split(": ", 1)[1].lower() == "true" for line in lines if line.startswith("Found Solution:")), False\n        )\n\n        if not reflections:\n            logging.warning("No reflections found in the response. Using default values.")\n            reflections = "No reflections provided."\n\n        return Reflection(reflections=reflections, score=score, found_solution=found_solution)\n    except Exception as e:\n        logging.error(f"Error in reflection_chain: {str(e)}", exc_info=True)\n        return Reflection(reflections=f"Error in reflection: {str(e)}", score=0, found_solution=False)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"initial-response",children:"Initial Response"}),"\n",(0,s.jsx)(n.p,{children:"We start with a single root node, generated by this first step. It\nresponds to the user input either with a tool invocation or a response."}),"\n",(0,s.jsx)(n.h1,{id:"create-autogen-agents",children:"Create Autogen agents"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'assistant = AssistantAgent(name="assistant", llm_config={"config_list": config_list}, code_execution_config=False)\nuser = UserProxyAgent(\n    name="user",\n    human_input_mode="NEVER",\n    max_consecutive_auto_reply=10,\n    code_execution_config={"work_dir": "web", "use_docker": False},\n)\n'})}),"\n",(0,s.jsx)(n.h1,{id:"define-a-function-to-create-the-initial-prompt",children:"Define a function to create the initial prompt"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def create_initial_prompt(input_text):\n    return [\n        {"role": "system", "content": "You are an AI assistant."},\n        {"role": "user", "content": input_text},\n    ]\n'})}),"\n",(0,s.jsx)(n.h1,{id:"function-to-generate-initial-response",children:"Function to generate initial response"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def generate_initial_response(state: TreeState) -> TreeState:\n    chat_messages = create_initial_prompt(state["input"])\n    try:\n        # Ensure chat_messages is a list of dictionaries\n        if not isinstance(chat_messages, list):\n            chat_messages = [{"role": "user", "content": chat_messages}]\n\n        logging.info(f"Generating initial response for input: {state[\'input\']}")\n        logging.debug(f"Chat messages: {chat_messages}")\n\n        response = assistant.generate_reply(chat_messages)\n        logging.debug(f"Raw response from assistant: {response}")\n\n        # Ensure response is properly formatted as a string\n        if isinstance(response, str):\n            content = response\n        elif isinstance(response, dict) and "content" in response:\n            content = response["content"]\n        elif isinstance(response, list) and len(response) > 0:\n            content = response[-1].get("content", str(response[-1]))\n        else:\n            content = str(response)\n\n        content = content.strip()\n        if not content:\n            raise ValueError("Generated content is empty after processing")\n\n        logging.debug(f"Processed content: {content[:100]}...")  # Log first 100 chars\n\n        # Generate reflection\n        reflection_input = {"input": state["input"], "candidate": content}\n        logging.info("Generating reflection on the initial response")\n        reflection = reflection_chain(reflection_input)\n        logging.debug(f"Reflection generated: {reflection}")\n\n        # Create Node with messages as a list containing a single dict\n        messages = [{"role": "assistant", "content": content}]\n        root = Node(messages=messages, reflection=reflection)\n\n        logging.info("Initial response and reflection generated successfully")\n        return TreeState(root=root, input=state["input"])\n\n    except Exception as e:\n        logging.error(f"Error in generate_initial_response: {str(e)}", exc_info=True)\n        return TreeState(root=None, input=state["input"])\n'})}),"\n",(0,s.jsx)(n.h1,{id:"example-usage-of-the-generate_initial_response-function",children:"Example usage of the generate_initial_response function"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'initial_prompt = "Why is the sky blue?"\ninitial_state = TreeState(input=initial_prompt, root=None)\nresult_state = generate_initial_response(initial_state)\nif result_state["root"] is not None:\n    print(result_state["root"].messages[0]["content"])\nelse:\n    print("Failed to generate initial response.")\n'})}),"\n",(0,s.jsx)(n.h4,{id:"starting-node",children:"Starting Node"}),"\n",(0,s.jsx)(n.p,{children:"We will package up the candidate generation and reflection in a single\nnode of our graph. This is represented by the following function:"}),"\n",(0,s.jsx)(n.h1,{id:"define-the-function-to-generate-the-initial-response",children:"Define the function to generate the initial response"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Define the function to generate the initial response\n\n\ndef generate_initial_response(state: TreeState) -> TreeState:\n    """Generate the initial candidate response using Autogen components."""\n    assistant = AssistantAgent(name="assistant", llm_config={"config_list": config_list}, code_execution_config=False)\n\n    # Generate initial response\n    initial_message = [\n        {"role": "system", "content": "You are an AI assistant."},\n        {"role": "user", "content": state["input"]},\n    ]\n\n    try:\n        logging.info(f"Generating initial response for input: {state[\'input\']}")\n        response = assistant.generate_reply(initial_message)\n        logging.debug(f"Raw response from assistant: {response}")\n\n        # Ensure response is properly formatted as a string\n        if isinstance(response, str):\n            content = response\n        elif isinstance(response, dict):\n            content = response.get("content", "")\n            if not content:\n                content = json.dumps(response)\n        elif isinstance(response, list):\n            content = " ".join(str(item) for item in response)\n        else:\n            content = str(response)\n\n        # Ensure content is always a string and not empty\n        content = content.strip()\n        if not content:\n            raise ValueError("Generated content is empty after processing")\n\n        logging.debug(f"Final processed content (first 100 chars): {content[:100]}...")\n\n        # Generate reflection\n        logging.info("Generating reflection on the initial response")\n        reflection_input = {"input": state["input"], "candidate": content}\n        reflection = reflection_chain(reflection_input)\n        logging.debug(f"Reflection generated: {reflection}")\n\n        if not isinstance(reflection, Reflection):\n            raise TypeError(f"Invalid reflection type: {type(reflection)}. Expected Reflection, got {type(reflection)}")\n\n        # Create Node with messages as a list containing a single dict\n        messages = [{"role": "assistant", "content": content}]\n        logging.debug(f"Creating Node with messages: {messages}")\n        root = Node(messages=messages, reflection=reflection)\n        logging.info("Initial response and reflection generated successfully")\n        logging.debug(f"Created root node: {root}")\n        return TreeState(root=root, input=state["input"])\n\n    except Exception as e:\n        logging.error(f"Error in generate_initial_response: {str(e)}", exc_info=True)\n        return TreeState(root=None, input=state["input"])\n'})}),"\n",(0,s.jsx)(n.h3,{id:"candidate-generation",children:"Candidate Generation"}),"\n",(0,s.jsx)(n.p,{children:"The following code prompts the same LLM to generate N additional\ncandidates to check."}),"\n",(0,s.jsx)(n.p,{children:"This generates N candidate values for a single input to sample actions\nfrom the environment"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def generate_candidates(messages: list, config: dict):\n    n = config.get("N", 5)\n    assistant = AssistantAgent(name="assistant", llm_config={"config_list": config_list}, code_execution_config=False)\n\n    candidates = []\n    for _ in range(n):\n        try:\n            # Use the assistant to generate a response\n            last_message = messages[-1]["content"] if messages and isinstance(messages[-1], dict) else str(messages[-1])\n            response = assistant.generate_reply([{"role": "user", "content": last_message}])\n            if isinstance(response, str):\n                candidates.append(response)\n            elif isinstance(response, dict) and "content" in response:\n                candidates.append(response["content"])\n            elif (\n                isinstance(response, list) and response and isinstance(response[-1], dict) and "content" in response[-1]\n            ):\n                candidates.append(response[-1]["content"])\n            else:\n                candidates.append(str(response))\n        except Exception as e:\n            logging.error(f"Error generating candidate: {str(e)}")\n            candidates.append("Failed to generate candidate.")\n\n    if not candidates:\n        logging.warning("No candidates were generated.")\n\n    return candidates\n\n\nexpansion_chain = generate_candidates\n'})}),"\n",(0,s.jsx)(n.h4,{id:"candidate-generation-node",children:"Candidate generation node"}),"\n",(0,s.jsx)(n.p,{children:"We will package the candidate generation and reflection steps in the\nfollowing \u201cexpand\u201d node. We do all the operations as a batch process to\nspeed up execution."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def expand(state: TreeState, config: Dict[str, Any]) -> dict:\n    root = state["root"]\n    best_candidate: Node = root.best_child if root.children else root\n    messages = best_candidate.get_trajectory()\n\n    # Generate N candidates using Autogen\'s generate_candidates function\n    new_candidates = generate_candidates(messages, config)\n\n    # Reflect on each candidate using Autogen\'s AssistantAgent\n    reflections = []\n    for candidate in new_candidates:\n        reflection = reflection_chain({"input": state["input"], "candidate": candidate})\n        reflections.append(reflection)\n\n    # Grow tree\n    child_nodes = [\n        Node([{"role": "assistant", "content": candidate}], parent=best_candidate, reflection=reflection)\n        for candidate, reflection in zip(new_candidates, reflections)\n    ]\n    best_candidate.children.extend(child_nodes)\n\n    # We have already extended the tree directly, so we just return the state\n    return state\n'})}),"\n",(0,s.jsx)(n.h2,{id:"create-tree",children:"Create Tree"}),"\n",(0,s.jsx)(n.p,{children:"With those two nodes defined, we are ready to define the tree. After\neach agent step, we have the option of finishing."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from typing import Any, Dict, Literal\n\n\ndef should_loop(state: Dict[str, Any]) -> Literal["expand", "end"]:\n    """Determine whether to continue the tree search."""\n    root = state["root"]\n    if root.is_solved:\n        return "end"\n    if root.height > 5:\n        return "end"\n    return "expand"\n\n\ndef run_lats(input_query: str, max_iterations: int = 10):\n    import logging\n\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n\n    try:\n\n        state = {"input": input_query, "root": None}\n        try:\n            state = generate_initial_response(state)\n            if not isinstance(state, dict) or "root" not in state or state["root"] is None:\n                logger.error("Initial response generation failed or returned invalid state")\n                return "Failed to generate initial response."\n            logger.info("Initial response generated successfully")\n        except Exception as e:\n            logger.error(f"Error generating initial response: {str(e)}", exc_info=True)\n            return "Failed to generate initial response due to an unexpected error."\n\n        for iteration in range(max_iterations):\n            action = should_loop(state)\n            if action == "end":\n                logger.info(f"Search ended after {iteration + 1} iterations")\n                break\n            try:\n                state = expand(\n                    state,\n                    {\n                        "N": 5,\n                        "input_query": input_query,\n                    },\n                )\n                logger.info(f"Completed iteration {iteration + 1}")\n            except Exception as e:\n                logger.error(f"Error during iteration {iteration + 1}: {str(e)}", exc_info=True)\n                continue\n\n        if not isinstance(state, dict) or "root" not in state or state["root"] is None:\n            return "No valid solution found due to an error in the search process."\n\n        solution_node = state["root"].get_best_solution()\n        best_trajectory = solution_node.get_trajectory(include_reflections=False)\n        if not best_trajectory:\n            return "No solution found in the search process."\n\n        result = (\n            best_trajectory[-1].get("content") if isinstance(best_trajectory[-1], dict) else str(best_trajectory[-1])\n        )\n        logger.info("LATS search completed successfully")\n        return result\n    except Exception as e:\n        logger.error(f"An unexpected error occurred during LATS execution: {str(e)}", exc_info=True)\n        return f"An unexpected error occurred: {str(e)}"\n'})}),"\n",(0,s.jsx)(n.p,{children:"Example usage:"}),"\n",(0,s.jsx)(n.p,{children:"result = run_lats(\u201cWrite a research report on deep learning.\u201d)"}),"\n",(0,s.jsx)(n.p,{children:"print(result)"}),"\n",(0,s.jsx)(n.h1,{id:"example-usage-of-the-lats-algorithm-with-autogen",children:"Example usage of the LATS algorithm with Autogen"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import logging\n\nlogging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")\nlogger = logging.getLogger(__name__)\n\n\ndef run_lats_example(question):\n    try:\n        logger.info(f"Processing question: {question}")\n        result = run_lats(question)\n        logger.info(f"LATS algorithm completed. Result: {result[:100]}...")  # Log first 100 chars of result\n        print(f"Question: {question}")\n        print(f"Answer: {result}")\n    except Exception as e:\n        logger.error(f"An error occurred while processing the question: {str(e)}", exc_info=True)\n        print(f"An error occurred: {str(e)}")\n    finally:\n        print("---")\n'})}),"\n",(0,s.jsx)(n.h1,{id:"list-of-example-questions",children:"List of example questions"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'questions = [\n    "Explain how epigenetic modifications can influence gene expression across generations and the implications for evolution.",\n    "Discuss the challenges of grounding ethical theories in moral realism, especially in light of the is-ought problem introduced by Hume.",\n    "How does the Riemann Hypothesis relate to the distribution of prime numbers, and why is it significant in number theory?",\n    "Describe the challenges and theoretical underpinnings of unifying general relativity with quantum mechanics, particularly focusing on string theory and loop quantum gravity.",\n]\n'})}),"\n",(0,s.jsx)(n.h1,{id:"run-lats-algorithm-for-each-question",children:"Run LATS algorithm for each question"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'for i, question in enumerate(questions, 1):\n    print(f"\\nExample {i}:")\n    run_lats_example(question)\n\nlogger.info("All examples processed.")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(n.p,{children:"Congrats on implementing LATS! This is a technique that can be\nreasonably fast and effective at solving complex agent tasks. A few\nnotes that you probably observed above:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"While LATS is effective, the tree rollout process can require\nadditional inference compute time. If you plan to integrate this\ninto a production application, consider streaming intermediate steps\nto allow users to see the thought process and access intermediate\nresults. Alternatively, you could use it to generate fine-tuning\ndata to enhance single-shot accuracy and avoid lengthy rollouts. The\ncost of using LATS has significantly decreased since its initial\nproposal and is expected to continue decreasing."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The effectiveness of the candidate selection process depends on the\nquality of the rewards generated. In this example, we exclusively\nuse self-reflection as feedback, but if you have access to external\nfeedback sources (such as code test execution), those should be\nincorporated as suggested above."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h1,{id:""})]})}function g(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>a,a:()=>r});var s=t(67294);const i={},o=s.createContext(i);function r(e){const n=s.useContext(o);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);